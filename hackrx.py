# -*- coding: utf-8 -*-
"""hackrx.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tnwvZEFPJMwoofJDLNdnunl5WjLM5zoS
"""

!pip install fastapi uvicorn pyngrok python-multipart sentence-transformers faiss-cpu pymupdf google-generativeai requests

# Commented out IPython magic to ensure Python compatibility.
# %%writefile main.py
# from fastapi import FastAPI, HTTPException, Request
# from pydantic import BaseModel
# from fastapi.responses import JSONResponse
# import requests
# import fitz  # PyMuPDF
# import os
# import uuid
# from sentence_transformers import SentenceTransformer
# import faiss
# import numpy as np
# import google.generativeai as genai
# 
# # --- Config ---
# genai.configure(api_key="")
# 
# model = SentenceTransformer('all-MiniLM-L6-v2')
# 
# app = FastAPI()
# 
# # --- Input/Output Models ---
# class QueryRequest(BaseModel):
#     pdf_url: str
#     questions: list[str]
# 
# class QueryResponse(BaseModel):
#     answers: dict
# 
# # --- Helper Functions ---
# def download_pdf_from_url(pdf_url):
#     try:
#         response = requests.get(pdf_url)
#         response.raise_for_status()
#         file_path = f"/tmp/{uuid.uuid4().hex}.pdf"
#         with open(file_path, "wb") as f:
#             f.write(response.content)
#         return file_path
#     except Exception as e:
#         raise HTTPException(status_code=400, detail=f"Failed to download PDF: {e}")
# 
# def extract_text_from_pdf(file_path):
#     try:
#         doc = fitz.open(file_path)
#         text = " ".join([page.get_text() for page in doc])
#         doc.close()
#         return text
#     except Exception as e:
#         raise HTTPException(status_code=500, detail=f"PDF extraction error: {e}")
# 
# def chunk_text(text, chunk_size=300, overlap=50):
#     import nltk
#     nltk.download("punkt")
#     from nltk.tokenize import sent_tokenize
# 
#     sentences = sent_tokenize(text)
#     chunks = []
#     chunk = ""
#     for sentence in sentences:
#         if len(chunk) + len(sentence) <= chunk_size:
#             chunk += " " + sentence
#         else:
#             chunks.append(chunk.strip())
#             chunk = sentence
#     if chunk:
#         chunks.append(chunk.strip())
# 
#     # Add overlap
#     final_chunks = []
#     for i in range(0, len(chunks), 1):
#         start = max(0, i - 1)
#         final_chunks.append(" ".join(chunks[start:i + 1]))
#     return final_chunks
# 
# def create_faiss_index(chunks):
#     embeddings = model.encode(chunks)
#     dim = embeddings.shape[1]
#     index = faiss.IndexFlatL2(dim)
#     index.add(np.array(embeddings))
#     return index, embeddings, chunks
# 
# def get_top_k_chunks(question, index, chunks, embeddings, k=5):
#     q_embedding = model.encode([question])
#     D, I = index.search(np.array(q_embedding), k)
#     return [chunks[i] for i in I[0]]
# 
# def generate_answer(context, question):
#     prompt = f"""
# Answer the following question using the provided context only.
# If the answer is not in the context, say "Not Found".
# 
# Context:
# {context}
# 
# Question:
# {question}
# """
#     model = genai.GenerativeModel("gemini-1.5-flash")
#     response = model.generate_content(prompt)
#     return response.text.strip()
# 
# # --- API Route ---
# @app.post("/query", response_model=QueryResponse)
# async def query_pdf(request: QueryRequest):
#     # Step 1: Download
#     file_path = download_pdf_from_url(request.pdf_url)
# 
#     # Step 2: Extract
#     full_text = extract_text_from_pdf(file_path)
# 
#     # Step 3: Chunk & Index
#     chunks = chunk_text(full_text)
#     index, embeddings, chunk_texts = create_faiss_index(chunks)
# 
#     # Step 4: Answer questions
#     result = {}
#     for question in request.questions:
#         top_chunks = get_top_k_chunks(question, index, chunk_texts, embeddings)
#         context = "\n".join(top_chunks)
#         answer = generate_answer(context, question)
#         result[question] = answer
# 
#     os.remove(file_path)  # Clean up
# 
#     return {"answers": result}

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# cat > requirements.txt <<'REQ'
# fastapi
# uvicorn[standard]
# pydantic
# requests
# PyMuPDF
# sentence-transformers
# faiss-cpu
# numpy
# google-generativeai
# nltk
# scikit-learn
# httpx
# REQ
# 
# cat > Procfile <<'PF'
# web: uvicorn main:app --host 0.0.0.0 --port $PORT
# PF
# 
# cat > .gitignore <<'GIT'
# __pycache__/
# *.pyc
# .env
# venv/
# .cache/
# models/
# *.sqlite
# GIT
#

# 1. Set up git identity
!git config --global user.name "aarush2109"
!git config --global user.email "aarushm2109@gmail.com"

# 2. Initialize git repo
!git init
!git add .
!git commit -m "Initial commit - HackRx API"

# 3. Link your GitHub repo (replace with your actual URL)
!git branch -M main
!git remote add origin https://github.com/aarush2109/hackrx-apitest.git

# 4. Push code to GitHub (will ask for GitHub username + personal access token)
!git push -u origin main

